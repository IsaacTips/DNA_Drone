{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab27b65-37d2-475e-a2a4-5d0a642ca335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ 2021-9-2 torch 1.9.0+cu102 CUDA:0 (Tesla V100-SXM2-32GB, 32510.5MB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': 'runs/train/exp/weights/best.pt', 'source': 'validation', 'img_size': 1280, 'conf_thres': 0.05, 'iou_thres': 0.45, 'device': '0', 'view_img': False, 'save_txt': False, 'save_conf': False, 'nosave': False, 'classes': None, 'agnostic_nms': False, 'augment': False, 'update': False, 'project': 'runs/detect', 'name': 'exp', 'exist_ok': False, 'domain': '../../datasets_ro/challenge-dataset/public'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 224 layers, 7053910 parameters, 0 gradients, 16.3 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "[i] Total Params: 7.05M\n",
      "[875/875] - ../../datasets_ro/challenge-dataset/public/images/0038_00362.jpg\n",
      "# Inference time per an image = 10ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.system('pip install easydict')\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized\n",
    "\n",
    "from utils.datasets import letterbox\n",
    "\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from eval_utils import mAP\n",
    "\n",
    "def create_directory(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "def read_txt(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "def detect(save_img=False):\n",
    "    source, weights, view_img, save_txt, imgsz = opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size\n",
    "\n",
    "    # Initialize\n",
    "    set_logging()\n",
    "    device = select_device(opt.device)\n",
    "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "    # Load model\n",
    "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "    stride = int(model.stride.max())  # model stride\n",
    "    imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
    "    if half:\n",
    "        model.half()  # to FP16\n",
    "\n",
    "    # Get names and colors\n",
    "    names = model.module.names if hasattr(model, 'module') else model.names\n",
    "    colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "\n",
    "    # Run inference\n",
    "    if device.type != 'cpu':\n",
    "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "\n",
    "    #######################################################################################################\n",
    "    # GYNetworks\n",
    "    #######################################################################################################\n",
    "    def calculate_parameters(model):\n",
    "        return sum(param.numel() for param in model.parameters())/1000000.0\n",
    "    print('[i] Total Params: %.2fM'%(calculate_parameters(model)))\n",
    "\n",
    "    root_dir = opt.domain\n",
    "    \n",
    "    image_paths = glob.glob(root_dir + '/images/*')\n",
    "\n",
    "    inference_times = []\n",
    "\n",
    "    f = open('./prediction.csv', 'w')\n",
    "\n",
    "    for index, image_path in enumerate(image_paths):\n",
    "        sys.stdout.write(f'\\r[{index + 1}/{len(image_paths)}] - {image_path}')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        image_name = os.path.basename(image_path)\n",
    "\n",
    "        img0 = cv2.imread(image_path)\n",
    "\n",
    "        st = time.time()\n",
    "\n",
    "        # Padded resize\n",
    "        img = letterbox(img0, imgsz, stride=stride)[0]\n",
    "\n",
    "        # Convert\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        img = img.to(device)\n",
    "        \n",
    "        # Inference\n",
    "        pred = model(img, augment=opt.augment)[0]\n",
    "\n",
    "        # Apply NMS\n",
    "        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
    "\n",
    "        # process prediction\n",
    "        # preds = [\n",
    "        #     # person\n",
    "        #     []\n",
    "        # ]\n",
    "\n",
    "        for i, det in enumerate(pred):  # detections per image\n",
    "            gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
    "\n",
    "                # Write results\n",
    "                for *xyxy, conf, class_index in reversed(det):\n",
    "                    # xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                    # line = (class_index, *xywh, conf)\n",
    "\n",
    "                    xyxy = torch.tensor(xyxy).view(1, 4).view(-1).numpy().astype(np.int32).tolist()  # normalized xywh\n",
    "\n",
    "                    class_index = int(class_index.item())\n",
    "\n",
    "                    confidence = conf.item()\n",
    "                    xmin, ymin, xmax, ymax = xyxy\n",
    "\n",
    "                    # preds[class_index].append([xmin, ymin, xmax, ymax, confidence])\n",
    "                    f.write('{},{:.0f},{:.0f},{:.0f},{:.0f},{:.2f}\\n'.format(image_name, xmin, ymin, xmax, ymax, confidence))\n",
    "\n",
    "        inference_time = int((time.time() - st) * 1000)\n",
    "        inference_times.append(inference_time)\n",
    "\n",
    "        # process gt\n",
    "        # height, width = img0.shape[:2]\n",
    "        # gt = []\n",
    "\n",
    "        # label_path = image_path.replace('/images', '/labels').replace('.jpg', '.txt')\n",
    "\n",
    "        # for data in open(label_path):\n",
    "        #     class_index, cx, cy, w, h = data.strip().split(' ')\n",
    "\n",
    "        #     class_index = int(class_index)\n",
    "        #     cx = int(float(cx) * width)\n",
    "        #     cy = int(float(cy) * height)\n",
    "        #     w = int(float(w) * width)\n",
    "        #     h = int(float(h) * height)\n",
    "\n",
    "        #     xmin = cx - w // 2\n",
    "        #     ymin = cy - h // 2\n",
    "        #     xmax = cx + w // 2\n",
    "        #     ymax = cy + h // 2\n",
    "\n",
    "        #     gt.append([xmin, ymin, xmax, ymax, class_index])\n",
    "\n",
    "        # visualize\n",
    "        # for data in preds[0]:\n",
    "        #     xmin, ymin, xmax, ymax = data[:4]\n",
    "        #     cv2.rectangle(img0, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "\n",
    "        # for data in gt:\n",
    "        #     xmin, ymin, xmax, ymax = data[:4]\n",
    "        #     cv2.rectangle(img0, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n",
    "\n",
    "        # cv2.imshow('Demo', img0)\n",
    "        # cv2.waitKey(0)\n",
    "        \n",
    "        # evaluator.add(preds, gt)\n",
    "    \n",
    "    #######################################################################################################\n",
    "    # _, aps = evaluator.calc_mean_ap()\n",
    "    # ap = aps[0]\n",
    "\n",
    "    inference_time = int(np.mean(inference_times[10:]))\n",
    "\n",
    "    print()\n",
    "    # print('# AP@0.50 = {:.2f}%'.format(ap * 100))\n",
    "    print('# Inference time per an image = {}ms'.format(inference_time))\n",
    "    print()\n",
    "\n",
    "    f.close()\n",
    "\n",
    "import easydict\n",
    "import sys\n",
    "if __name__ == '__main__':\n",
    "    opt = easydict.EasyDict({\n",
    "        'weights': 'runs/train/exp/weights/best.pt', # ÏÇ¨Ïö©Ïûê ÌïôÏäµ Î™®Îç∏(pt) Í≤ΩÎ°ú ÏûÖÎ†•\n",
    "        'source': 'validation',\n",
    "        'img_size': 1280,\n",
    "        'conf_thres': 0.05,\n",
    "        'iou_thres': 0.45,\n",
    "        'device': '0',\n",
    "        'view_img': False,\n",
    "        'save_txt': False,\n",
    "        'save_conf': False,\n",
    "        'nosave': False,\n",
    "        'classes': None,\n",
    "        'agnostic_nms': False,\n",
    "        'augment': False,\n",
    "        'update': False,\n",
    "        'project': 'runs/detect',\n",
    "        'name': 'exp',\n",
    "        'exist_ok': False,\n",
    "        'domain': '{HOME}/datasets_ro/challenge-dataset/public'.format(HOME=os.environ[\"HOME\"])\n",
    "    })\n",
    "    \n",
    "    print(opt)\n",
    "    # check_requirements(exclude=('pycocotools', 'thop'))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if opt.update:  # update all models (to fix SourceChangeWarning)\n",
    "            for opt.weights in ['yolov5s.pt', 'yolov5m.pt', 'yolov5l.pt', 'yolov5x.pt']:\n",
    "                detect()\n",
    "                strip_optimizer(opt.weights)\n",
    "        else:\n",
    "            detect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2986f3f8-38ef-4fff-addc-fc47dce1229d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
